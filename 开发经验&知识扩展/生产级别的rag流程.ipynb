{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19be88ab",
   "metadata": {},
   "source": [
    "## 初步设计的RAG实现流程如下\n",
    "\n",
    "### 存储流程\n",
    "\n",
    "* **原始文件** (text | pdf | excel | ...)\n",
    "➔ **解析**: `ingestion`\n",
    "➔ **切片**: `text_splitter`\n",
    "➔ **向量&存储**: `embedding & store`\n",
    "\n",
    "### 使用RAG流程\n",
    "\n",
    "* **用户问题** ➔ **query & 重写**: `multi-query retrieval`\n",
    "➔ **检索top-k**: `50`\n",
    "➔ **精排**: `cross encoder`\n",
    "➔ **LLM生成回答**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102602d5",
   "metadata": {},
   "source": [
    "### 切片"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a852c",
   "metadata": {},
   "source": [
    "\n",
    "`RecursiveCharacterTextSplitter` 是 LangChain 提供的文本切分工具：\n",
    "\n",
    "* **核心思想**：先按大单位切，再按小单位切，保证语义不被切断。\n",
    "* **作用**：\n",
    "\n",
    "  * 将长文本拆成 chunk\n",
    "  * 支持 overlap（防止边界信息丢失）\n",
    "  * 输出带 metadata 的列表，直接可做 embedding\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ef53070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "split into 2 chunks\n",
      "chunk 0: C) 上下文（Context）：提供与任务有关的背景信息。这有助于 LLM 理解正在讨论的具体场景，从而确保其响应是相关的。\n",
      "O) 目标（Objective）：定义你希望 LLM 执行的任务。明晰目标有助于 LLM 将自己响应重点放在完成具体任务上。\n",
      "S) 风格（Style）：指定你希望 LLM 使用的写作风格。这可能是一位具体名人的写作风格，也可以是某种职业专家（比如商业分析师或 CEO）的风格。这能引导 LLM 使用符合你需求的方式和词语给出响应。\n",
      "T) 语气（Tone）：设定响应的态度。这能确保 LLM 的响应符合所需的情感或情绪上下文，比如正式、幽默、善解人意等。\n",
      "A) 受众（Audience）：确定响应的目标受众。针对具体受众（比如领域专家、初学者、孩童）定制 LLM 的响应，确保其在你所需的上下文中是适当的和可被理解的。\n",
      "R) 响应（Response）：提供响应的格式。这能确保 LLM 输出你的下游任务所需的格式，比如列表、JSON、专业报告等。对于大多数通过程序化方法将 LLM 响应用于下游任务的 LLM 应用而言，理想的输出格式是 JSON。\n",
      "CO-STAR 的一个实际应用\n",
      "chunk 1: CO-STAR 的一个实际应用\n",
      "假设你是一位社交媒体管理者，你需要帮助草拟一篇 Facebook 帖文，其內容是宣传你公司的新产品。\n",
      "如果不使用 CO-STAR，那么你可能会使用这样的 prompt：\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text = \"\"\"\n",
    "C) 上下文（Context）：提供与任务有关的背景信息。这有助于 LLM 理解正在讨论的具体场景，从而确保其响应是相关的。\n",
    "O) 目标（Objective）：定义你希望 LLM 执行的任务。明晰目标有助于 LLM 将自己响应重点放在完成具体任务上。\n",
    "S) 风格（Style）：指定你希望 LLM 使用的写作风格。这可能是一位具体名人的写作风格，也可以是某种职业专家（比如商业分析师或 CEO）的风格。这能引导 LLM 使用符合你需求的方式和词语给出响应。\n",
    "T) 语气（Tone）：设定响应的态度。这能确保 LLM 的响应符合所需的情感或情绪上下文，比如正式、幽默、善解人意等。\n",
    "A) 受众（Audience）：确定响应的目标受众。针对具体受众（比如领域专家、初学者、孩童）定制 LLM 的响应，确保其在你所需的上下文中是适当的和可被理解的。\n",
    "R) 响应（Response）：提供响应的格式。这能确保 LLM 输出你的下游任务所需的格式，比如列表、JSON、专业报告等。对于大多数通过程序化方法将 LLM 响应用于下游任务的 LLM 应用而言，理想的输出格式是 JSON。\n",
    "CO-STAR 的一个实际应用\n",
    "假设你是一位社交媒体管理者，你需要帮助草拟一篇 Facebook 帖文，其內容是宣传你公司的新产品。\n",
    "如果不使用 CO-STAR，那么你可能会使用这样的 prompt：\n",
    "\"\"\"\n",
    "print(\"\\n\" in text)\n",
    "# 创建一个切片器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,   # 每个chunk的大小  实战建议，中文知识库 300~500之间\n",
    "    chunk_overlap=20,  # chunk之间的重叠  实战建议，中文知识库 50~100之间\n",
    "    length_function=len,  # 计算长度的函数\n",
    "    is_separator_regex=False,  # 是否将分隔符视为文本\n",
    "    separators=[\"\\n\\n\", \"\\n\", \"。\", \"！\", \"？\", \". \", \"! \", \"? \", \"；\", \"; \", \"，\", \", \", \" \", \"\"],\n",
    ")\n",
    "chunks = text_splitter.split_text(text)\n",
    "# 因为文本长度大于chunk_size，所以会切成多个chunk, chunk之间有重叠\n",
    "print(f\"split into {len(chunks)} chunks\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"chunk {i}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabd36b7",
   "metadata": {},
   "source": [
    "3️⃣ 切分工具的选择原则\n",
    "\n",
    "| 文档类型            | 推荐切分器                          | 备注         |\n",
    "| --------------- | ------------------------------ | ---------- |\n",
    "| PDF / 长手册       | RecursiveCharacterTextSplitter | 保留语义完整     |\n",
    "| Markdown / Wiki | MarkdownHeaderTextSplitter     | 按章节切       |\n",
    "| 新闻 / 小说         | SentenceSplitter               | 按句子切       |\n",
    "| 对话 / 聊天记录       | SentenceSplitter + 自定义轮次       | 保留发言单位     |\n",
    "| LLM 模型 token 控制 | TokenTextSplitter              | 精确控制 token |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aab155",
   "metadata": {},
   "source": [
    "### 计算文本向量\n",
    "文本向量化（Text Embedding）就是把原始文本/问题变成向量的步骤。\n",
    "1. 语义表示\n",
    "\n",
    "    - 原始文本（段落、文档、问题等）是字符串，不利于计算机理解语义。\n",
    "\n",
    "    - 通过模型（如 all-MiniLM-L6-v2）把文本转成 向量（embedding），向量 保留了文本的语义信息。\n",
    "\n",
    "    - 相似意思的文本 → 向量距离近；不同意思的文本 → 向量距离远。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcce17be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\64216\\PycharmProjects\\生产级rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1640.45it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: C:/huggingface/models/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02038683369755745, 0.025280876085162163, -0.0005662108305841684, 0.011615470983088017, -0.037988435477018356, -0.11998122185468674, 0.041709478944540024, -0.0208571907132864, -0.05900675430893898, 0.024232514202594757, 0.0621202327311039, 0.06767991185188293, 0.033100299537181854, -0.010369348339736462, -0.03121573105454445, -0.0327332429587841, -0.0021117725409567356, 0.00926193781197071, -0.12476459890604019, 0.011236855760216713, 0.03904542326927185, 0.05440250784158707, -0.0028255144134163857, 0.04455628618597984, -0.0854201540350914, -0.02287364937365055, 0.03914055600762367, 0.03604690730571747, -0.032126713544130325, -0.06425869464874268, 0.058129098266363144, 0.046690817922353745, 0.08061561733484268, -0.007734282407909632, -0.022083161398768425, 0.06713153421878815, -0.045041415840387344, -0.10212117433547974, 0.001264422433450818, 0.04680192843079567, 0.026395857334136963, -0.06990955024957657, -0.04453349485993385, -0.006901932880282402, 0.019288646057248116, 0.02059083990752697, 0.006518161855638027, 0.03549392148852348, 0.10393311828374863, 0.017503725364804268, -0.0429428331553936, -0.057037316262722015, -0.011423494666814804, 0.009236734360456467, 0.04582151770591736, 0.007003591395914555, 0.024210026487708092, -0.06064571440219879, -0.014943978749215603, -0.030515769496560097, -0.06836123019456863, 0.057068612426519394, -0.032270655035972595, 0.04119708761572838, 0.09017683565616608, -0.07689839601516724, -0.02232888713479042, 0.026091355830430984, -0.057754434645175934, -0.060503147542476654, -0.043829575181007385, 0.010114419274032116, 0.03421919792890549, 0.07573983818292618, -0.04518905654549599, 0.005837555043399334, 0.018490763381123543, -0.0018646095413714647, 0.017705973237752914, 0.05494632571935654, 0.06722185760736465, -0.10008066147565842, 0.01773884892463684, 0.04324394464492798, 0.010778208263218403, -0.014706453308463097, -0.013241097331047058, -0.001782219740562141, -0.04542683809995651, -0.03418901935219765, -0.14636534452438354, -0.01115794014185667, -0.011241843923926353, 0.01174066960811615, -0.08864283561706543, -0.028394129127264023, 0.07532476633787155, -0.01844591274857521, -0.1703874170780182, 0.15587182343006134, 0.022921405732631683, 0.046667180955410004, 0.04001077637076378, 0.02375500462949276, 0.04980279132723808, 0.03032159060239792, 0.0003741151886060834, 0.06957262009382248, -0.022312667220830917, -0.027472838759422302, 0.006083938758820295, -0.0485323928296566, 0.04923882335424423, -0.007612183224409819, 0.06917714327573776, -0.07174894213676453, -0.020257245749235153, 0.014374659396708012, -0.030236825346946716, 0.00418047932907939, 0.053489211946725845, -0.05887245759367943, 0.023056652396917343, 0.013102804310619831, 0.010882028378546238, 0.023222412914037704, 0.028361162170767784, -3.8436666931492404e-33, 0.04356621950864792, -0.0035945556592196226, 0.04212310165166855, 0.12318186461925507, 0.017473354935646057, 0.009427400305867195, -0.0945146456360817, -0.02123844064772129, 0.034263886511325836, 0.02595912292599678, 0.02806120179593563, 0.012698456645011902, -0.04617798700928688, 0.030305491760373116, -0.04523097723722458, 0.11220848560333252, -0.09135957062244415, -0.013798604719340801, 0.025815201923251152, 0.08335629105567932, -0.07693812251091003, -0.010359476320445538, 0.009555548429489136, 0.08872873336076736, -0.009140624664723873, 0.008417361415922642, 0.010792110115289688, -0.0907164141535759, 0.09623932093381882, 0.007239802274852991, -0.03825901448726654, -0.05111750587821007, 0.020446278154850006, 0.01577538251876831, -0.005840197671204805, 0.011155523359775543, -0.007191231939941645, -0.07329272478818893, -0.07283009588718414, -0.00611039949581027, -0.059314094483852386, 0.04546378552913666, 0.04360096901655197, -0.007337668910622597, -0.025582559406757355, -0.03440641611814499, 0.02559276670217514, 0.01813693158328533, 0.04025290533900261, 0.0399746373295784, -0.04333766549825668, 0.008319417014718056, -0.0388362854719162, 0.055851515382528305, -0.010561035946011543, 0.016997406259179115, 0.04742541164159775, -0.04800346866250038, -0.013104835525155067, 0.04660714045166969, -0.003912203013896942, 0.10242762416601181, -0.04255148395895958, -0.02821984700858593, -0.008180600591003895, -0.01885274052619934, 0.05203329399228096, 0.03386804088950157, 0.05951100215315819, 0.004061595536768436, -0.019567491486668587, 0.026742612943053246, 0.020931795239448547, 0.021920403465628624, 0.012750843539834023, 0.053985174745321274, 0.05206805095076561, -0.0031074071303009987, 0.02487240731716156, -0.07944539189338684, 0.02861769311130047, -0.0007746106130070984, -0.0033817547373473644, -0.051787324249744415, 0.09358304738998413, 0.018984464928507805, -0.009582627564668655, -0.0856575146317482, -0.0174981988966465, -0.0041584172286093235, -0.06506021320819855, 0.05912621691823006, 0.03576966002583504, -0.005036798771470785, -0.08909006416797638, 2.5757042832714254e-33, 0.1397932916879654, 0.017513636499643326, -0.05452437698841095, -0.0671004056930542, -0.01024399884045124, -0.032303206622600555, -0.07818872481584549, 0.14000578224658966, -0.07843431085348129, 0.04743696004152298, 0.021780453622341156, 0.021539777517318726, 0.12622776627540588, 0.025801103562116623, 0.022561803460121155, -0.01523616723716259, 0.13175281882286072, 0.014995912089943886, 0.014494232833385468, -0.0018083536997437477, -0.013143658638000488, -0.04916450381278992, -0.06190982088446617, 0.021932421252131462, -0.022566059604287148, 0.024125922471284866, 0.04778728634119034, 0.0013614906929433346, -0.12093909829854965, 0.013258966617286205, -0.015382491052150726, 0.028439344838261604, -0.031059565022587776, -0.014658544212579727, -0.016496190801262856, 0.023634182289242744, -0.09657490998506546, -0.03889469802379608, -0.02935652807354927, -0.03114948607981205, -0.04675929620862007, 0.010851340368390083, -0.006681278347969055, 0.0305335596203804, -0.10486806929111481, -0.0056226844899356365, -0.03426205739378929, 0.014524447731673717, -0.036871809512376785, -0.03581423684954643, -0.09492851793766022, -0.05121380835771561, 0.08636805415153503, -0.02769477292895317, -0.03255050629377365, 0.033519234508275986, -0.02360818162560463, -0.003329183906316757, 0.03848700225353241, -0.011646338738501072, 0.012732202187180519, 0.059461697936058044, 0.03451533615589142, 0.08603376895189285, 0.025225235149264336, -0.034104347229003906, 0.01370937842875719, 0.01557581964880228, 0.030829915776848793, -0.018169084563851357, 0.007548472844064236, 0.00767800398170948, -0.020997310057282448, -0.016836490482091904, -0.032185595482587814, 0.0636659488081932, 0.0030277203768491745, -0.01919613964855671, 0.01796713098883629, 0.030703285709023476, -0.010722174309194088, 0.0567406564950943, 0.02326798439025879, 0.029091529548168182, 0.007758197374641895, 0.06784669309854507, 0.08166714012622833, 0.047504544258117676, -0.026240697130560875, -0.042831771075725555, -0.009907637722790241, 0.006457664538174868, 0.0173024944961071, 0.03067108243703842, -0.0380118191242218, -1.6864360574686543e-08, -0.08774776011705399, 0.039147719740867615, -0.007313709706068039, 0.05522017553448677, 0.03042866289615631, 0.01835988089442253, -0.0877668559551239, -0.06734009087085724, -0.07474605739116669, -0.00930698961019516, 0.03774426504969597, 0.13193337619304657, -0.08082900941371918, 0.013214131817221642, 0.048574961721897125, 0.09028725326061249, -0.0293662641197443, 0.039682988077402115, -0.0341360867023468, 0.0035193569492548704, -0.011343846097588539, 0.009339170530438423, 0.01123310998082161, -0.06465622037649155, 0.03457619994878769, -0.09496654570102692, -0.007475678808987141, 0.0036895645316690207, 0.01051430031657219, -0.06667263805866241, 0.05160503834486008, 0.10477926582098007, -0.0547863207757473, 0.021519212052226067, -0.08572056889533997, -0.027919640764594078, 0.027237543836236, 0.0962936207652092, 0.0670931488275528, -0.07181669026613235, -0.09750433266162872, 0.04430793598294258, -0.053962744772434235, -0.10748161375522614, -0.05498868227005005, 0.03482292592525482, 0.06672010570764542, -0.056024547666311264, 0.02175181917846203, -0.06315213441848755, -0.06730662286281586, 0.037822380661964417, 0.07897450029850006, 0.002572552300989628, 0.10580895096063614, 0.09685953706502914, 0.047380004078149796, 0.03066212125122547, -0.008867024444043636, 0.06080888956785202, 0.030900953337550163, -0.030652424320578575, 0.03755693882703781, 0.03742789849638939]\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # ========== 国内镜像配置 ==========\n",
    "# os.environ[\"HF_HOME\"] = \"C:/huggingface\"             # 缓存目录，可改\n",
    "# os.environ[\"TRANSFORMERS_CACHE\"] = \"C:/huggingface\"\n",
    "# os.environ[\"HF_HUB_URL\"] = \"https://huggingface.tuna.tsinghua.edu.cn\"  # 清华镜像\n",
    "\n",
    "# ========== 本地模型路径 ==========\n",
    "local_model_path = \"C:/huggingface/models/all-MiniLM-L6-v2\"\n",
    "\n",
    "# # 如果模型还没下载，可以使用 snapshot_download 下载一次\n",
    "# from huggingface_hub import snapshot_download\n",
    "# snapshot_download(\n",
    "#     repo_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "#     local_dir=local_model_path\n",
    "# )\n",
    "\n",
    "# ========== 加载模型 ==========\n",
    "model = SentenceTransformer(local_model_path)\n",
    "\n",
    "# ========== 生成文本向量 ==========\n",
    "embeddings = model.encode(\"Hello world!\")\n",
    "\n",
    "# 输出\n",
    "print(embeddings.tolist())\n",
    "print(len(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac8111",
   "metadata": {},
   "source": [
    "### 向量存储"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f65a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1️⃣ FAISS 里只存：\n",
    "* 向量矩阵（float32）\n",
    "* 添加顺序（隐式索引 id）\n",
    "不存文本、不存 metadata。\n",
    "\n",
    "2️⃣ 文本和 metadata 存在：\n",
    "```python\n",
    "data.pkl\n",
    "```\n",
    "里面是：\n",
    "* `documents` 列表\n",
    "* `metadatas` 列表\n",
    "\n",
    "3️⃣ 它们靠 **数组下标一致**关联？(顺序绑定， 还有一种IndexID绑定比较复杂，适合生产级别使用)\n",
    "\n",
    "```\n",
    "FAISS id 0  ↔ documents[0]\n",
    "FAISS id 1  ↔ documents[1]\n",
    "FAISS id 2  ↔ documents[2]\n",
    "```\n",
    "FAISS 返回的索引 = 列表下标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbd4e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "texts = [\"apple\", \"banana\", \"orange\"]  # 文本数据\n",
    "vectors = [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 1.0]]  # 假设的对应的向量\n",
    "metadatas = [{\"type\": \"fruit\"}, {\"type\": \"fruit\"}, {\"type\": \"fruit\"}]  # 元数据\n",
    "path = \"./vector_store\"\n",
    "os.makedirs(path, exist_ok=True)\n",
    "dimension = 3\n",
    "index = faiss.IndexFlatL2(dimension)  # 创建了一个3维的L2距离的空的 FAISS index\n",
    "index.add(np.array(vectors).astype(\"float32\"))  # 把向量加入 index\n",
    "# ==== 存储到磁盘 =====\n",
    "faiss.write_index(index, os.path.join(path, \"index.faiss\"))  \n",
    "with open(os.path.join(path, \"data.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"documents\": texts,\n",
    "        \"metadatas\": metadatas,\n",
    "        \"dimension\": 3\n",
    "    }, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1716d3",
   "metadata": {},
   "source": [
    "### （openrouter + 个人VPS + ccr）对接使用llm\n",
    "\n",
    "> 这里我就不能过多描述了...就是避免直接官方API（国内没有办法） 使用中转站 + 个人VPS + Claude Code Router 实现对接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1173e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today? Is there something I can help you with?\n"
     ]
    }
   ],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"CLAUDE_CODE_API_KEY\")\n",
    "base_url = os.getenv(\"CLAUDE_CODE_BASE_URL\")\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model='claude-sonnet-4.5',\n",
    "    temperature=0.3,\n",
    "    anthropic_api_key=api_key,\n",
    "    base_url=base_url\n",
    ")\n",
    "message = llm.invoke(\"hello\")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e233881e",
   "metadata": {},
   "source": [
    "实践通过， 继续我们的RAG使用的流程\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6115c00",
   "metadata": {},
   "source": [
    "\n",
    "## 问题\n",
    "\n",
    "### 为什么要切片？\n",
    "\n",
    "**1. 存储流程中，所选用的向量模型是有长度限制的**\n",
    "\n",
    "> 几乎所有的向量模型都有严格的输入长度限制，如 `bge-large-zh`：512个token、OpenAI 的 `text-embedding-ada-002` 支持 8192 个 token。\n",
    "\n",
    "**2. 切片的核心目标是：保证检索的语义精准度**\n",
    "\n",
    "> 向量化 (Embedding) 的本质是把一段文段压缩成一个几百维的浮点数组。这个数组代表了这段话的“核心主题”，之后我们在检索的时候就依赖于这个核心主题进行相似度计算。\n",
    "> * **如果切片大小合适**（比如一段话或一页）：这个向量能精准代表这一段的具体细节（比如“2023年Q3苹果公司大中华区营收数据”）。\n",
    "> * **如果不切片**（整本书算一个向量）：整本书的信息被强行揉捏在一起，就像把满汉全席放进榨汁机打碎一样，最后得到的向量代表的是一个极度模糊的全局概念（比如“一本关于商业的书”）。当你提问具体的细节时，系统根本无法通过相似度匹配找到正确的内容，因为细节的语义完全被长文本里的“噪音”淹没了。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "### 单从知识的角度理解什么是“完美切片”\n",
    "\n",
    "最理想的切片方法，本质并不是 **切 (split)** 而是 **重写与提炼 (rewrite & extract)**。\n",
    "\n",
    "它是把长文档打碎后，重新捏成一个个哪怕完全脱离原文档，也能让路人（或者大模型）一眼看懂的**独立知识块**。\n",
    "\n",
    "> ps: 目前业界为了达到这种“理想状态”，正在流行一种叫做 Contextual Retrieval（上下文检索） 的前沿方案，也就是让大模型先帮每个切片加上背景信息再存入数据库。\n",
    "\n",
    "\n",
    "**【案例演示】**\n",
    "\n",
    "假设你的知识库中有一篇题为《苹果 2023 WWDC 大会纪要》的文档，其中有一段原始文本如下：\n",
    "\n",
    "> “该公司在当天的发布会上正式推出了首款空间计算设备。它定价3499美元，预计明年发售。CEO蒂姆表示，这不仅是一款新硬件，更是全人类生活方式的革命。为了保证它的产能，团队已经向索尼追加了百万块屏幕的订单。”\n",
    "\n",
    "#### ❌ 灾难现场：物理切片 (Physical Split)\n",
    "\n",
    "如果你使用传统的切分器（比如按标点符号或按 50 个字符硬切），你会得到以下切片：\n",
    "\n",
    "* **Chunk 1:** “该公司在当天的发布会上正式推出了首款空间计算设备。它定价3499美元，预计明年发售。”\n",
    "* **Chunk 2:** “CEO蒂姆表示，这不仅是一款新硬件，更是全人类生活方式的革命。”\n",
    "* **Chunk 3:** “为了保证它的产能，团队已经向索尼追加了百万块屏幕的订单。”\n",
    "\n",
    "**痛点分析：** 假设用户提问：“苹果 Vision Pro 的屏幕是谁供应的？” 系统会去搜索，但 Chunk 3 里只有“它”、“团队”和“索尼”。因为丢失了上下文，向量模型根本不知道“它”就是 Vision Pro，“团队”就是苹果团队。这个包含标准答案的切片，在检索阶段就会被无情淘汰。\n",
    "\n",
    "#### ✅ 降维打击：重写与提炼 (Rewrite & Extract)\n",
    "\n",
    "如果采用理想的“信息胶囊”模式（利用大模型在入库前进行指代消解和实体补全），长文档被打碎后，会被重新捏成完全独立的知识块：\n",
    "\n",
    "* **知识块 1:** “【苹果2023发布会】苹果公司推出了首款空间计算设备（Vision Pro），该设备定价为3499美元，预计于2024年发售。”\n",
    "* **知识块 2:** “【苹果2023发布会】苹果公司CEO蒂姆·库克表示，Vision Pro的发布不仅是推出新硬件，更是全人类生活方式的革命。”\n",
    "* **知识块 3:** “【苹果2023发布会】为了保证空间计算设备Vision Pro的产能，苹果团队已经向索尼公司追加了百万块屏幕的订单。”\n",
    "\n",
    "**优势分析：** 现在，你随便拿出一个知识块扔在马路上，捡到它的路人（或大模型）看一眼就能明白完整的意思。没有让人抓狂的“它”、“这”、“该公司”。当用户再问“苹果 Vision Pro 的屏幕是谁供应的？”时，知识块 3 包含了“苹果”、“Vision Pro”、“产能”、“索尼”、“屏幕”等所有核心实体，向量匹配度会直接爆表，大模型也能基于此给出完美回答。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
